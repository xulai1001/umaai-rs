# 剧本类型: "basic" (无剧本) | "onsen" (温泉剧本)
scenario = "onsen"

# 日志级别: "debug" (完整显示) | "off" (全部关闭) | "info" (简要显示) | "trace" (详细显示) 
log_level = "off"

# 训练员类型:
#   "manual"      - 手动选择（交互式，不支持多次模拟）
#   "random"      - 猴子训练员（随机选择，用于基准测试）
#   "handwritten" - 手写策略（快速，使用启发式规则）
#   "collector"   - 样本收集器（用于生成神经网络训练数据）
#   "neuralnet"   - 神经网络训练员（使用 ONNX 模型进行决策）
#   "mcts"        - 蒙特卡洛训练员（使用手写逻辑+mcts进行决策）

# 【样本收集模式说明】最初版本，马上就会改了
# 当 trainer = "collector" 时
#   1. 运行 simulation_count 次模拟
#   2. 使用手写逻辑进行决策
#   3. 收集每回合的游戏状态和决策
#   4. 按最终分数排序，筛选 Top 1% 最强赛马娘精英样本
#   5. 保存到 training_data.bin 文件
# 运行命令: cargo run --release --bin umasim
trainer = "mcts"

# MCTS 配置（仅当 trainer = "mcts" 时生效）
#   search_n              - 搜索次数，每个动作模拟次数（默认128，推荐1024）
#   radical_factor_max    - 激进度上限，越高越倾向高风险高回报（默认50）
#   max_depth             - 最大搜索深度，0=搜到游戏结束（默认0）
#   policy_delta          - 策略温度（训练数据生成时使用，默认100）
#
# UCB 搜索分配参数 
#   use_ucb               - 是否启用 UCB 分配（默认 false）
#   search_group_size     - UCB 每次给动作增加的搜索次数（默认256牢大是1024）
#   search_cpuct          - UCB 探索常数，越大越倾向探索搜索次数少的动作（默认1.0）
#   expected_search_stdev - 预期搜索标准差（默认2200,牢大的我直接抄了我也不懂什么意思）
#   adjust_radical_by_turn - 激进度随回合递减，后期更稳（默认 true）
#
#   ⬇️🐧⭐ toml 的内联表{ ..... }不支持换行 ，换行会报错。
mcts = { search_n = 1024, radical_factor_max = 50.0, max_depth = 0, policy_delta = 100.0, adjust_radical_by_turn = true, use_ucb = true, search_group_size = 256 }

# 模拟次数（默认1次，大于1时显示最高分/最低分面板和平均分）

simulation_count = 10

# 马娘ID
uma = 106301

# 卡组(ID, 突破等级)
# 温泉剧本需要包含友人卡 (chara_id = 9050)
cards = [302754, 302654, 302744, 302644, 302774, 302764]

# 种马蓝因子个数 [速度, 耐力, 力量, 根性, 智力]
blue_count = [12, 0, 0, 0, 6]

# 种马额外属性 [速度, 耐力, 力量, 根性, 智力, 技能点]
extra_count = [10, 40, 0, 0, 40, 50]

# 温泉选择顺序 
# 疾驰之泉(1) - 速度/力量友情
# 坚忍之泉(2) - 耐力/根性友情
# 明晰之泉(3) - 比赛+30%
# 骏闪古泉(4) - Hint+100%
# 刚足古泉(5) - 力量/根性友情\
# 健壮古泉(6) - 耐力/体力消耗
# 天翔古泉(7) - 比赛+60%
# 秘汤汤驹(8) - 分身效果
# 传说秘泉(9) - 比赛+80%（⭐特殊：第三年9月强制选择）
onsen_order = [
    1, 3, 2,    # 第一年
    5, 7, 4,    # 第二年
    8, 9,       # 第三年
    4, 2, 6     # 补漏用
]